{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf07a1c",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "- Antibody usage by checkpoint\n",
    "- Interface length and residue frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03bddb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns #for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "import requests\n",
    "from Bio.PDB import PDBParser, NeighborSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets for individual inhibitory checkpoints\n",
    "data = pd.read_csv('data/ab_ag.tsv', sep='\\t')\n",
    "pd1 = data[data['compound'].str.contains('PD-1', na=False)]\n",
    "pdl1 = data[data['compound'].str.contains('PD-L1', na=False)]\n",
    "ctla4 = data[data['compound'].str.contains('CTLA-4', na=False)]\n",
    "kir = data[data['compound'].str.contains('KIR', na=False)]\n",
    "lag3 = data[data['compound'].str.contains('LAG', na=False)]\n",
    "tim3 = data[data['compound'].str.contains('TIM3', na=False)]\n",
    "\n",
    "\n",
    "# Filter out necessary columns\n",
    "pd1 = pd1[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "pdl1 = pdl1[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "ctla4 = ctla4[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "kir = kir[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "lag3 = lag3[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "tim3 = tim3[['pdb', 'Hchain', 'Lchain', 'antigen_chain', 'antigen_type', 'antigen_name', 'compound', 'resolution', 'method']]\n",
    "\n",
    "\n",
    "combined_dataset = [pdl1, pd1, ctla4, kir, lag3, tim3]\n",
    "\n",
    "# Shuffle the list randomly\n",
    "random.shuffle(combined_dataset)\n",
    "\n",
    "# Concatenate them in the new random order\n",
    "merged_df = pd.concat(combined_dataset, ignore_index=True)\n",
    "shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "shuffled_df = shuffled_df[shuffled_df['resolution'] < 3.5]\n",
    "shuffled_df = shuffled_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb134fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: 8u31.pdb\n",
      "Downloaded: 8u32.pdb\n",
      "Downloaded: 8gy5.pdb\n",
      "Downloaded: 7wsl.pdb\n",
      "Downloaded: 8as0.pdb\n",
      "Downloaded: 5ggs.pdb\n",
      "Downloaded: 7cgw.pdb\n",
      "Downloaded: 5jxe.pdb\n",
      "Downloaded: 5ggr.pdb\n",
      "Downloaded: 5b8c.pdb\n",
      "Downloaded: 6jjp.pdb\n",
      "Downloaded: 5wt9.pdb\n",
      "Downloaded: 7bxa.pdb\n",
      "Downloaded: 7e9b.pdb\n",
      "Downloaded: 7wvm.pdb\n",
      "Downloaded: 7cu5.pdb\n",
      "Downloaded: 6k0y.pdb\n",
      "Downloaded: 6xkr.pdb\n",
      "Downloaded: 8rpb.pdb\n",
      "Downloaded: 5xj4.pdb\n",
      "Downloaded: 7yds.pdb\n",
      "Downloaded: 5ggt.pdb\n",
      "Downloaded: 5grj.pdb\n",
      "Downloaded: 5x8l.pdb\n",
      "Downloaded: 5x8m.pdb\n",
      "Downloaded: 5xxy.pdb\n",
      "Downloaded: 9dq3.pdb\n",
      "Downloaded: 9dq4.pdb\n",
      "Downloaded: 9dq5.pdb\n",
      "Downloaded: 5xj3.pdb\n",
      "Downloaded: 6rp8.pdb\n",
      "Downloaded: 7dv4.pdb\n",
      "Downloaded: 5ggv.pdb\n",
      "Downloaded: 7su0.pdb\n",
      "Downloaded: 5tru.pdb\n",
      "Downloaded: 6xy2.pdb\n",
      "Downloaded: 7su1.pdb\n",
      "Downloaded: 7elx.pdb\n",
      "Downloaded: 8tui.pdb\n",
      "Downloaded: 7tzh.pdb\n",
      "Downloaded: 7tzg.pdb\n",
      "Downloaded: 6txz.pdb\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['pd1', 'pdl1', 'ctla4', 'kir', 'lag3', 'tim3']\n",
    "\n",
    "pdb_ids = {\n",
    "    'pd1': pd1['pdb'].unique().tolist(),\n",
    "    'pdl1': pdl1['pdb'].unique().tolist(),\n",
    "    'ctla4': ctla4['pdb'].unique().tolist(),\n",
    "    'kir': kir['pdb'].unique().tolist(),\n",
    "    'lag3': lag3['pdb'].unique().tolist(),\n",
    "    'tim3': tim3['pdb'].unique().tolist(),\n",
    "}\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    save_folder = f'structures/{checkpoint}' # Define save directory\n",
    "    os.makedirs(save_folder, exist_ok=True) # make new directory with name above if doesn't exist\n",
    "    for pdb_id in pdb_ids[checkpoint]:\n",
    "        pdb_id = pdb_id.lower()\n",
    "        url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(save_folder, f\"{pdb_id}.pdb\"), \"w\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded: {pdb_id}.pdb\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {pdb_id}.pdb (status code: {response.status_code})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fea912ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_interface_residues(pdb_path, heavy_chain, light_chain, antigen_chain, cutoff=5.0):\n",
    "    pdb_id = os.path.basename(pdb_path).replace(\".pdb\", \"\")\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_id, pdb_path)\n",
    "    model = structure[0]\n",
    "\n",
    "    ab_chains = [heavy_chain, light_chain]\n",
    "    ag_chain = antigen_chain\n",
    "\n",
    "    ab_atoms, ag_atoms = [], []\n",
    "    ab_residues, ag_residues = set(), set()\n",
    "\n",
    "    for chain in model:\n",
    "        if chain.id in ab_chains:\n",
    "            ab_atoms.extend(chain.get_atoms())\n",
    "        elif chain.id == ag_chain:\n",
    "            ag_atoms.extend(chain.get_atoms())\n",
    "\n",
    "    ns = NeighborSearch(ab_atoms + ag_atoms)\n",
    "\n",
    "    for atom in ab_atoms:\n",
    "        for neighbor in ns.search(atom.coord, cutoff):\n",
    "            res = neighbor.get_parent()\n",
    "            if res.get_parent().id == ag_chain:\n",
    "                ag_residues.add(res)\n",
    "\n",
    "    for atom in ag_atoms:\n",
    "        for neighbor in ns.search(atom.coord, cutoff):\n",
    "            res = neighbor.get_parent()\n",
    "            if res.get_parent().id in ab_chains:\n",
    "                ab_residues.add(res)\n",
    "\n",
    "    rows = []\n",
    "    # Count frequency\n",
    "    ab_freq = Counter(res.resname for res in ab_residues)\n",
    "    ag_freq = Counter(res.resname for res in ag_residues)\n",
    "\n",
    "    for res in ab_residues:\n",
    "        rows.append({\n",
    "            \"pdb_id\": pdb_id,\n",
    "            \"chain_id\": res.get_parent().id,\n",
    "            \"partner_type\": \"antibody\",\n",
    "            \"residue_name\": res.resname,\n",
    "            \"residue_number\": res.id[1],\n",
    "            \"residue_id\": f\"{res.id[1]}{res.id[2].strip()}\",\n",
    "            \"chain_pair\": f\"{heavy_chain}{light_chain}-{antigen_chain}\",\n",
    "            \"interface_distance\": cutoff,\n",
    "            \"residue_frequency\": ab_freq[res.resname]\n",
    "        })\n",
    "\n",
    "    for res in ag_residues:\n",
    "        rows.append({\n",
    "            \"pdb_id\": pdb_id,\n",
    "            \"chain_id\": res.get_parent().id,\n",
    "            \"partner_type\": \"antigen\",\n",
    "            \"residue_name\": res.resname,\n",
    "            \"residue_number\": res.id[1],\n",
    "            \"residue_id\": f\"{res.id[1]}{res.id[2].strip()}\",\n",
    "            \"chain_pair\": f\"{heavy_chain}{light_chain}-{antigen_chain}\",\n",
    "            \"interface_distance\": cutoff,\n",
    "            \"residue_frequency\": ag_freq[res.resname]\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved detailed data to pdl1_interface_residues.csv\n",
      "✅ Saved pivot summary to pdl1_residue_frequencies_pivot.csv\n"
     ]
    }
   ],
   "source": [
    "current_checkpoint = 'pdl1' # wechseln\n",
    "folder_path = f\"structures/{current_checkpoint}\"\n",
    "ckpt_ids = os.listdir(folder_path)\n",
    "all_interface_rows = []\n",
    "\n",
    "for ckpt_id in ckpt_ids:\n",
    "    pdb_path = os.path.join(folder_path, ckpt_id)\n",
    "    pdb_id = ckpt_id.replace(\".pdb\", \"\")\n",
    "\n",
    "    try:\n",
    "        H = pdl1[pdl1['pdb'] == pdb_id]['Hchain'].values[0] # wechseln \n",
    "        L = pdl1[pdl1['pdb'] == pdb_id]['Lchain'].values[0] # wechseln\n",
    "        A = pdl1[pdl1['pdb'] == pdb_id]['antigen_chain'].values[0] # wechseln\n",
    "\n",
    "        rows = analyze_interface_residues(pdb_path, H, L, A)\n",
    "        all_interface_rows.extend(rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {ckpt_id}: {e}\")\n",
    "\n",
    "# === Save full long-format table ===\n",
    "df_out = pd.DataFrame(all_interface_rows)\n",
    "output_folder = f\"data/{current_checkpoint}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "df_out.to_csv(f\"{output_folder}/{current_checkpoint}_interface_residues.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Saved detailed data to {current_checkpoint}_interface_residues.csv\")\n",
    "\n",
    "\n",
    "# === Create pivot summary ===\n",
    "df_pivot = df_out.groupby(['residue_name', 'pdb_id'])['residue_name'].count().unstack(fill_value=0)\n",
    "df_pivot.to_csv(f\"{output_folder}/{current_checkpoint}_residue_frequencies_pivot.csv\")\n",
    "\n",
    "print(f\"✅ Saved pivot summary to {current_checkpoint}_residue_frequencies_pivot.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
